{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Sinhala Grammar and Spelling Checker\n",
        "\n",
        "By:-\n",
        "\n",
        "THENUWARA T.H.T.P      (2020/E/192)\n",
        "\n",
        "WICKRAMARCHCHI V.D.    (2020/E/217)"
      ],
      "metadata": {
        "id": "JAZ_SqxFpiq-"
      },
      "id": "JAZ_SqxFpiq-"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAnOAcjidNkX",
        "outputId": "f9aa03ef-fae0-446e-fd4d-2425fe60b381"
      },
      "id": "aAnOAcjidNkX",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a35b30d8-fe70-4305-be39-f8b9bf43cc24",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a35b30d8-fe70-4305-be39-f8b9bf43cc24",
        "outputId": "7cf9a897-c83b-4417-a665-9376bf41f030"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 67260 correct Sinhala words.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load Excel dataset\n",
        "dictionary_path = \"/content/drive/MyDrive/UOJ/SEM 7/AI/Sinhala_Grammar_and_Spell_Checker/data-spell-checker.xlsx\"\n",
        "data = pd.read_excel(dictionary_path)\n",
        "\n",
        "# Extract correct words\n",
        "dictionary = data[data['label'] == 1]['word'].tolist()\n",
        "print(f\"Loaded {len(dictionary)} correct Sinhala words.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bb6e0fac-42d2-493d-a590-21ef0155bde6",
      "metadata": {
        "id": "bb6e0fac-42d2-493d-a590-21ef0155bde6"
      },
      "outputs": [],
      "source": [
        "# Load Sinhala stop words\n",
        "stopwords_path = \"/content/drive/MyDrive/UOJ/SEM 7/AI/Sinhala_Grammar_and_Spell_Checker/stop words.txt\"\n",
        "with open(stopwords_path, 'r', encoding='utf-8') as file:\n",
        "    stopwords = set(file.read().splitlines())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4affbe2b-693f-4bff-99ce-18bd23af4218",
      "metadata": {
        "id": "4affbe2b-693f-4bff-99ce-18bd23af4218"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def preprocess_text(text, stopwords):\n",
        "    # Use regex to remove non-alphanumeric characters except spaces\n",
        "    cleaned_text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation (keeping spaces)\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    words = cleaned_text.split()\n",
        "\n",
        "    # Remove stop words\n",
        "    filtered_words = [word for word in words if word not in stopwords]\n",
        "    return filtered_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ec106113-e843-4ff6-a223-39284d6dd3c5",
      "metadata": {
        "id": "ec106113-e843-4ff6-a223-39284d6dd3c5"
      },
      "outputs": [],
      "source": [
        "def sinhala_stemmer(word):\n",
        "    suffixes = ['ින්', 'ට', 'ව', 'ගේ', 'යන්', 'නවා']  # Add more relevant suffixes\n",
        "    for suffix in suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            return word[:-len(suffix)]\n",
        "    return word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8fbd918d-7188-49f8-9cbc-3c0283a4af26",
      "metadata": {
        "id": "8fbd918d-7188-49f8-9cbc-3c0283a4af26"
      },
      "outputs": [],
      "source": [
        "def detect_spelling_errors(words, dictionary):\n",
        "    # Find words not in the dictionary\n",
        "    errors = [word for word in words if word not in dictionary]\n",
        "    return errors\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uh5p9Rqwelbt",
        "outputId": "3f0cbfdd-f2ef-4753-c717-d759ad5a18d6"
      },
      "id": "uh5p9Rqwelbt",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cd125c02-89c5-4c4a-a712-38f17e8927e1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd125c02-89c5-4c4a-a712-38f17e8927e1",
        "outputId": "8e2aa3c2-96ee-4d71-8774-e7076858ab5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
          ]
        }
      ],
      "source": [
        "from fuzzywuzzy import process\n",
        "\n",
        "def sinhala_soundex(word):\n",
        "    phonetic_map = {\n",
        "        'ක': '1', 'ඛ': '1',\n",
        "        'ච': '2', 'ජ': '2', 'ඡ': '2', 'ඣ': '2',\n",
        "        'ට': '3', 'ඩ': '3', 'ඨ': '3', 'ඪ': '3',\n",
        "        'ත': '4', 'ථ': '4',\n",
        "        'බ': '5', 'ඵ': '5', 'භ': '5',\n",
        "        'ශ': '7', 'ෂ': '7', 'ස': '7',\n",
        "        'ග': '8', 'ඝ': '8', 'ඟ': '8'\n",
        "    }\n",
        "    first_letter = word[0]\n",
        "    soundex_code = [first_letter]\n",
        "    for char in word[1:]:\n",
        "        if char in phonetic_map:\n",
        "            code = phonetic_map[char]\n",
        "            if soundex_code[-1] != code:\n",
        "                soundex_code.append(code)\n",
        "    while len(soundex_code) < 4:\n",
        "        soundex_code.append('0')  # Pad with zeros\n",
        "    return ''.join(soundex_code[:4])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7692da54-010d-491a-a0d1-da1ea6938c8d",
      "metadata": {
        "id": "7692da54-010d-491a-a0d1-da1ea6938c8d"
      },
      "outputs": [],
      "source": [
        "def suggest_corrections(errors, dictionary, limit=3, threshold=80):\n",
        "    suggestions = {}\n",
        "    for error in errors:\n",
        "        matches = process.extract(error, dictionary, limit=limit)\n",
        "        # Filter suggestions based on minimum similarity threshold\n",
        "        filtered_matches = [match[0] for match in matches if match[1] >= threshold]\n",
        "        suggestions[error] = filtered_matches\n",
        "    return suggestions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "38c33d4c-a1f5-45ab-b35a-adf6fd325a21",
      "metadata": {
        "id": "38c33d4c-a1f5-45ab-b35a-adf6fd325a21"
      },
      "outputs": [],
      "source": [
        "def spell_checker(input_text, dictionary, stopwords):\n",
        "    # Step 1: Preprocess input text\n",
        "    words = preprocess_text(input_text, stopwords)\n",
        "    # Step 2: Detect spelling errors\n",
        "    errors = detect_spelling_errors(words, dictionary)\n",
        "\n",
        "    if not errors:\n",
        "        return \"No spelling errors found!\", {}\n",
        "\n",
        "    # Step 3: Suggest corrections for detected errors\n",
        "    corrections = suggest_corrections(errors, dictionary)\n",
        "    return errors, corrections\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "328936b9-98af-461d-83a7-9ceb36d1cf2b",
      "metadata": {
        "id": "328936b9-98af-461d-83a7-9ceb36d1cf2b"
      },
      "outputs": [],
      "source": [
        "def auto_correct(input_text, dictionary, stopwords):\n",
        "    errors, corrections = spell_checker(input_text, dictionary, stopwords)\n",
        "    words = input_text.split()\n",
        "\n",
        "    # Replace each word with the top suggestion if available\n",
        "    corrected_words = [\n",
        "        corrections.get(word, [word])[0]  # If word is found in corrections, replace it\n",
        "        for word in words\n",
        "    ]\n",
        "\n",
        "    return \" \".join(corrected_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "408667bd-6f31-44e2-b7fb-ce156f12c52e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "408667bd-6f31-44e2-b7fb-ce156f12c52e",
        "outputId": "38b849f2-a4e1-45f4-b432-b2c63f451c44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Errors: ['කකල']\n",
            "Suggestions: {'කකල': ['කලා', 'ලා', 'ලිං']}\n",
            "Corrected Text: කලා\n"
          ]
        }
      ],
      "source": [
        "input_text = \"කකල\"    #කකුල\n",
        "\n",
        "# Run the spell checker\n",
        "errors, corrections = spell_checker(input_text, dictionary, stopwords)\n",
        "print(\"Errors:\", errors)\n",
        "print(\"Suggestions:\", corrections)\n",
        "\n",
        "# Auto-correct the text\n",
        "corrected_text = auto_correct(input_text, dictionary, stopwords)\n",
        "print(\"Corrected Text:\", corrected_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9046ac9e-0808-4291-bd24-77dc19a8d5f3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9046ac9e-0808-4291-bd24-77dc19a8d5f3",
        "outputId": "6dd6a743-e6b3-42eb-d926-7a552e957794"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spelling Errors: {'සරභුමිය': [('සාරභුමිය', 89.1), ('සරිය', 73.0), ('සරුවිය', 72.5), ('රැමිය', 71.8), ('සැමිය', 71.8)]}\n",
            "Corrected Text: සාරභුමිය\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from fuzzywuzzy import fuzz, process\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "class AdvancedSinhalaSpellChecker:\n",
        "    def __init__(self, dictionary_path: str, stopwords_path: str):\n",
        "        # Load dictionary with more sophisticated preprocessing\n",
        "        self.data = pd.read_excel(dictionary_path)\n",
        "        self.dictionary = self._preprocess_dictionary()\n",
        "\n",
        "        # Load stopwords\n",
        "        with open(stopwords_path, 'r', encoding='utf-8') as file:\n",
        "            self.stopwords = set(file.read().splitlines())\n",
        "\n",
        "        # Advanced phonetic mapping\n",
        "        self.phonetic_mapping = self._create_advanced_phonetic_mapping()\n",
        "\n",
        "        # Suffix rules for more comprehensive stemming\n",
        "        self.suffix_rules = [\n",
        "            'ආගම', 'ගෙන', 'යෙහි', 'යේ', 'ට', 'ම',\n",
        "            'යන', 'ක', 'වා', 'ලා', 'ල', 'න', 'හි'\n",
        "        ]\n",
        "\n",
        "    def _preprocess_dictionary(self) -> List[str]:\n",
        "\n",
        "        # Assuming 'word' column contains correct words and 'label' column indicates correctness\n",
        "        correct_words = self.data[self.data['label'] == 1]['word']\n",
        "\n",
        "        # Remove duplicates, convert to lowercase, remove special characters\n",
        "        processed_words = set(\n",
        "            re.sub(r'[^\\u0D80-\\u0DFF]', '', word.lower())\n",
        "            for word in correct_words\n",
        "        )\n",
        "\n",
        "        return list(processed_words)\n",
        "\n",
        "    def _create_advanced_phonetic_mapping(self) -> Dict[str, str]:\n",
        "\n",
        "        return {\n",
        "            # Consonant groups with similar sounds\n",
        "            'ක': 'k', 'ඛ': 'k', 'ගෑ': 'g', 'ඝ': 'g',\n",
        "            'ච': 'c', 'ජ': 'j', 'ඣ': 'j',\n",
        "            'ට': 't', 'ඩ': 'd', 'ඨ': 't', 'ඪ': 'd',\n",
        "            'ත': 't', 'ද': 'd', 'ධ': 'd',\n",
        "            'ප': 'p', 'බ': 'b', 'භ': 'b',\n",
        "            'ම': 'm', 'න': 'n', 'ණ': 'n',\n",
        "            'ල': 'l', 'ළ': 'l',\n",
        "            'ර': 'r', 'ඍ': 'r',\n",
        "            'ව': 'v', 'ශ': 's', 'ෂ': 's', 'ස': 's',\n",
        "            'හ': 'h'\n",
        "        }\n",
        "\n",
        "    def _advanced_stemmer(self, word: str) -> str:\n",
        "\n",
        "        original_word = word\n",
        "        for suffix in self.suffix_rules:\n",
        "            if word.endswith(suffix):\n",
        "                word = word[:-len(suffix)]\n",
        "                break\n",
        "\n",
        "        # If no suffix removed and word is too short, return original\n",
        "        return word if len(word) > 2 else original_word\n",
        "\n",
        "    def _phonetic_key(self, word: str) -> str:\n",
        "\n",
        "        phonetic_key = ''\n",
        "        for char in word:\n",
        "            phonetic_key += self.phonetic_mapping.get(char, char)\n",
        "        return phonetic_key\n",
        "\n",
        "    def find_corrections(self, word: str, limit: int = 5, threshold: int = 70) -> List[Tuple[str, int]]:\n",
        "\n",
        "        if word in self.stopwords or word in self.dictionary:\n",
        "            return [(word, 100)]\n",
        "\n",
        "        # Stemming\n",
        "        stemmed_word = self._advanced_stemmer(word)\n",
        "\n",
        "        # Multiple similarity strategies\n",
        "        candidates = []\n",
        "        for dict_word in self.dictionary:\n",
        "            # Phonetic similarity\n",
        "            phonetic_similarity = fuzz.ratio(\n",
        "                self._phonetic_key(stemmed_word),\n",
        "                self._phonetic_key(dict_word)\n",
        "            )\n",
        "\n",
        "            # String-based similarity\n",
        "            string_similarity = fuzz.ratio(word, dict_word)\n",
        "\n",
        "            # Levenshtein distance\n",
        "            edit_similarity = fuzz.token_sort_ratio(word, dict_word)\n",
        "\n",
        "            # Combined weighted similarity\n",
        "            combined_score = (\n",
        "                0.4 * phonetic_similarity +\n",
        "                0.3 * string_similarity +\n",
        "                0.3 * edit_similarity\n",
        "            )\n",
        "\n",
        "            candidates.append((dict_word, combined_score))\n",
        "\n",
        "        # Sort and filter candidates\n",
        "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "        return [\n",
        "            (candidate, score)\n",
        "            for candidate, score in candidates\n",
        "            if score >= threshold\n",
        "        ][:limit]\n",
        "\n",
        "    def spell_check(self, text: str) -> Dict[str, List[Tuple[str, int]]]:\n",
        "\n",
        "        # Preprocess text\n",
        "        words = re.findall(r'\\S+', text)\n",
        "\n",
        "        # Spelling error detection and correction\n",
        "        spelling_errors = {}\n",
        "        for word in words:\n",
        "            if word not in self.dictionary and word not in self.stopwords:\n",
        "                corrections = self.find_corrections(word)\n",
        "                if corrections:\n",
        "                    spelling_errors[word] = corrections\n",
        "\n",
        "        return spelling_errors\n",
        "\n",
        "    def auto_correct(self, text: str) -> str:\n",
        "\n",
        "        errors = self.spell_check(text)\n",
        "        corrected_words = []\n",
        "\n",
        "        for word in text.split():\n",
        "            if word in errors:\n",
        "                # Take the first (best) suggestion\n",
        "                corrected_words.append(errors[word][0][0])\n",
        "            else:\n",
        "                corrected_words.append(word)\n",
        "\n",
        "        return ' '.join(corrected_words)\n",
        "\n",
        "# Example Usage\n",
        "def main():\n",
        "    # Initialize spell checker\n",
        "    spell_checker = AdvancedSinhalaSpellChecker(\n",
        "        dictionary_path='/content/drive/MyDrive/UOJ/SEM 7/AI/Sinhala_Grammar_and_Spell_Checker/data-spell-checker.xlsx',\n",
        "        stopwords_path='/content/drive/MyDrive/UOJ/SEM 7/AI/Sinhala_Grammar_and_Spell_Checker/stop words.txt'\n",
        "    )\n",
        "\n",
        "    # Test input\n",
        "    test_text = \"සරභුමිය\"\n",
        "\n",
        "    # Spell check\n",
        "    spelling_errors = spell_checker.spell_check(test_text)\n",
        "    print(\"Spelling Errors:\", spelling_errors)\n",
        "\n",
        "    # Auto-correction\n",
        "    corrected_text = spell_checker.auto_correct(test_text)\n",
        "    print(\"Corrected Text:\", corrected_text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "4553d2dd-89a5-4276-944d-fe47b2793680",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4553d2dd-89a5-4276-944d-fe47b2793680",
        "outputId": "dd2c8904-0923-4156-9546-19c5ee31e858"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sinling\n",
            "  Downloading sinling-0.3.6-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting emoji (from sinling)\n",
            "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sinling) (1.4.2)\n",
            "Collecting pygtrie (from sinling)\n",
            "  Downloading pygtrie-2.5.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting sklearn-crfsuite (from sinling)\n",
            "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sinling) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sinling) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->sinling) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->sinling) (4.67.1)\n",
            "Collecting python-crfsuite>=0.9.7 (from sklearn-crfsuite->sinling)\n",
            "  Downloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->sinling) (1.6.0)\n",
            "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->sinling) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->sklearn-crfsuite->sinling) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->sklearn-crfsuite->sinling) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->sklearn-crfsuite->sinling) (3.5.0)\n",
            "Downloading sinling-0.3.6-py3-none-any.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pygtrie-2.5.0-py3-none-any.whl (25 kB)\n",
            "Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pygtrie, python-crfsuite, emoji, sklearn-crfsuite, sinling\n",
            "Successfully installed emoji-2.14.0 pygtrie-2.5.0 python-crfsuite-0.9.11 sinling-0.3.6 sklearn-crfsuite-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install sinling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "8906f93e-b8b4-4663-a1de-7635d1404507",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8906f93e-b8b4-4663-a1de-7635d1404507",
        "outputId": "e04b5c64-ea8c-43e9-dfcd-be87d4613e2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('අපි', 'PRP'), ('බත්', 'NNC'), ('කනවා', 'VFM'), ('.', 'FS')]]\n"
          ]
        }
      ],
      "source": [
        "from sinling import SinhalaTokenizer, POSTagger\n",
        "\n",
        "tokenizer = SinhalaTokenizer()\n",
        "tagger = POSTagger()\n",
        "\n",
        "document = \"අපි බත් කනවා\"\n",
        "tokenized_sentences = [tokenizer.tokenize(f'{ss}.') for ss in tokenizer.split_sentences(document)]\n",
        "pos_tags = tagger.predict(tokenized_sentences)\n",
        "print(pos_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "763f9a3a-0cbc-4ed0-ba0a-ee3c668ff417",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "763f9a3a-0cbc-4ed0-ba0a-ee3c668ff417",
        "outputId": "44e83136-e14c-45d5-8ac2-e1ebe356665f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Spell Check for: අභිචෝදකයා නඩුව සාක්ෂි ලබාදුන්නා. අංකනය කිරීම සඳහා කාර්යමණ්ඩලය වැඩි වේලාවක් ගත කළා. අංකාන්තරය සොයා ගැනීම සඳහා විශේෂ පරීක්ෂණයක කරනු ලැබුවා.\n",
            "Spelling Errors: {'කාර්යමණ්ඩලය': [('කාර්ය්යමණ්ඩලය', 84.82727272727273)], 'වේලාවක්': [('වේලාසන', 75.5), ('වේලනවා', 71.25)], 'කළා.': [('කළා', 82.62857142857143), ('කළාලය', 70.05)], 'ගැනීම': [('ගැන', 82.62857142857143), ('ගැනුම', 75.0)], 'පරීක්ෂණයක': [('පරීක්ෂණය', 92.5), ('පරීක්ෂය', 85.77692307692308), ('පරීක්ෂා', 85.77692307692308), ('පරීරක්ෂණය', 82.73333333333333), ('පරීක්ෂාව', 81.42857142857143)], 'කරනු': [('කරු', 77.15357142857144), ('කරුව', 77.15357142857144), ('කරන්නා', 75.0)]}\n",
            "Auto-corrected Text: අභිචෝදකයා නඩුව සාක්ෂි ලබාදුන්නා. අංකනය කිරීම සඳහා කාර්ය්යමණ්ඩලය වැඩි වේලාසන ගත කළා අංකාන්තරය සොයා ගැන සඳහා විශේෂ පරීක්ෂණය කරු ලැබුවා.\n",
            "\n",
            "--- Spell Check for: මම කඩේට ගියෙමු. අප සෙල්ලම් කළෝය.\n",
            "Spelling Errors: {'කඩේට': [('කඩ', 77.0)], 'කළෝය.': [('කළය', 70.42500000000001)]}\n",
            "Auto-corrected Text: මම කඩ ගියෙමු. අප සෙල්ලම් කළය\n",
            "\n",
            "--- Spell Check for: උස ගසක් තිබෙයි. කුරුල්ලා ඒ මත කූඩුවක් තමයි. කුරුලු පැටව කෑම කයි\n",
            "Spelling Errors: {'කූඩුවක්': [('කූඩුව', 81.02222222222221), ('කූඩය', 71.25)], 'පැටව': [('පැටවා', 92.5), ('පැටියා', 82.62857142857143), ('පැටලෙනවා', 72.9), ('පැටලීම', 72.9), ('පැටවීම', 72.9)], 'කයි': [('කයින්', 92.5), ('කොයි', 77.15357142857144), ('කය', 77.0), ('කයිරුව', 70.05), ('කයිප්පු', 70.05)]}\n",
            "Auto-corrected Text: උස ගසක් තිබෙයි. කුරුල්ලා ඒ මත කූඩුව තමයි. කුරුලු පැටවා කෑම කයින්\n",
            "\n",
            "--- Spell Check for: පාර්ලිමේන්තුවේ අද විශෂ සාකච්ඡාව පැවැත්විණි. පාර්ලිමේනතු මන්ත්‍රීවරයා ජාතික ප්‍රශ්න කතාබහ කළා.\n",
            "Spelling Errors: {'පාර්ලිමේන්තුවේ': [('පාර්ලිමේන්තුව', 92.5), ('පාර්ලිමේන්තු', 89.61304347826088)], 'විශෂ': [('විශේෂය', 83.12222222222222), ('විශේෂ', 83.12222222222222), ('විශේෂී', 83.12222222222222), ('විශද', 80.52857142857142), ('විෂය', 78.05357142857143)], 'පැවැත්විණි.': [('පැවැත්වීම', 72.42105263157895)], 'පාර්ලිමේනතු': [('පාර්ලිමේන්තු', 86.9595238095238), ('පාර්ලිමේන්තුව', 84.07727272727273)], 'මන්ත්\\u200dරීවරයා': [('මන්ත්රකාරයා', 72.57105263157894), ('මන්ත්රය', 72.0)], 'ප්\\u200dරශ්න': [('ප්රශ්නය', 75.20833333333334), ('ප්රකාශ්ය', 70.90576923076924)], 'කළා.': [('කළා', 82.62857142857143), ('කළාලය', 70.05)]}\n",
            "Auto-corrected Text: පාර්ලිමේන්තුව අද විශේෂය සාකච්ඡාව පැවැත්වීම පාර්ලිමේන්තු මන්ත්රකාරයා ජාතික ප්රශ්නය කතාබහ කළා\n",
            "\n",
            "--- Spell Check for: මගේ ඉඩම විශාල ප්‍රමාණයකි. මගේ ගෘහ නිවසයේ වැඩක් ඉටු කළා.\n",
            "Spelling Errors: {'ප්\\u200dරමාණයකි.': [('ප්රමාණක', 73.51944444444445)], 'නිවසයේ': [('නිවස', 92.5), ('නිවසන', 92.5), ('නිවෙස', 83.12222222222222), ('නිවාස', 83.12222222222222), ('නිවාසය', 83.12222222222222)], 'වැඩක්': [('වැඩය', 92.5), ('වැඩ', 92.5), ('වැඩේ', 92.5), ('වැඩෙනවා', 82.62857142857143), ('වැඩිම', 82.62857142857143)], 'කළා.': [('කළා', 82.62857142857143), ('කළාලය', 70.05)]}\n",
            "Auto-corrected Text: මගේ ඉඩම විශාල ප්රමාණක මගේ ගෘහ නිවස වැඩය ඉටු කළා\n"
          ]
        }
      ],
      "source": [
        "import difflib\n",
        "\n",
        "class SinhalaSpellChecker:\n",
        "    def __init__(self, dictionary_path: str, stopwords_path: str, suffixes_path: str):\n",
        "        # Load dictionary\n",
        "        self.data = pd.read_excel(dictionary_path)\n",
        "        self.dictionary = self._preprocess_dictionary()\n",
        "\n",
        "        # Load stopwords\n",
        "        with open(stopwords_path, 'r', encoding='utf-8') as file:\n",
        "            self.stopwords = set(file.read().splitlines())\n",
        "\n",
        "        # Load suffixes from suffixes_list.txt\n",
        "        with open(suffixes_path, 'r', encoding='utf-8') as file:\n",
        "            self.suffix_rules = file.read().splitlines()\n",
        "\n",
        "        # Advanced phonetic mapping\n",
        "        self.phonetic_mapping = self._create_advanced_phonetic_mapping()\n",
        "\n",
        "        # Prefix and suffix variations\n",
        "        self.prefix_variations = {\n",
        "            'අ': ['ආ', 'අ'],\n",
        "            'අද': ['ආද', 'අද'],\n",
        "            'අන': ['ආන', 'අන']\n",
        "        }\n",
        "\n",
        "    def _preprocess_dictionary(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        Advanced dictionary preprocessing\n",
        "        \"\"\"\n",
        "        correct_words = self.data[self.data['label'] == 1]['word']\n",
        "\n",
        "        # Remove duplicates, convert to lowercase, remove special characters\n",
        "        processed_words = set(\n",
        "            re.sub(r'[^\\u0D80-\\u0DFF]', '', word.lower())\n",
        "            for word in correct_words\n",
        "        )\n",
        "\n",
        "        return list(processed_words)\n",
        "\n",
        "    def _create_advanced_phonetic_mapping(self) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Comprehensive phonetic mapping for Sinhala characters\n",
        "        \"\"\"\n",
        "        return {\n",
        "            # Consonant groups with similar sounds\n",
        "            'ක': 'k', 'ඛ': 'k', 'ගෑ': 'g', 'ඝ': 'g',\n",
        "            'ච': 'c', 'ජ': 'j', 'ඣ': 'j',\n",
        "            'ට': 't', 'ඩ': 'd', 'ඨ': 't', 'ඪ': 'd',\n",
        "            'ත': 't', 'ද': 'd', 'ධ': 'd',\n",
        "            'ප': 'p', 'බ': 'b', 'භ': 'b',\n",
        "            'ම': 'm', 'න': 'n', 'ණ': 'n',\n",
        "            'ල': 'l', 'ළ': 'l',\n",
        "            'ර': 'r', 'ඍ': 'r',\n",
        "            'ව': 'v', 'ශ': 's', 'ෂ': 's', 'ස': 's',\n",
        "            'හ': 'h'\n",
        "        }\n",
        "\n",
        "    def _advanced_stemmer(self, word: str) -> str:\n",
        "        \"\"\"\n",
        "        Advanced stemming with multiple suffix removal strategies\n",
        "        \"\"\"\n",
        "        original_word = word\n",
        "        for suffix in self.suffix_rules:\n",
        "            if word.endswith(suffix):\n",
        "                word = word[:-len(suffix)]\n",
        "                break\n",
        "\n",
        "        # If no suffix removed and word is too short, return original\n",
        "        return word if len(word) > 2 else original_word\n",
        "\n",
        "    def _phonetic_key(self, word: str) -> str:\n",
        "        \"\"\"\n",
        "        Generate advanced phonetic key\n",
        "        \"\"\"\n",
        "        phonetic_key = ''\n",
        "        for char in word:\n",
        "            phonetic_key += self.phonetic_mapping.get(char, char)\n",
        "        return phonetic_key\n",
        "\n",
        "    def _generate_prefix_variations(self, word: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Generate potential prefix variations of a word\n",
        "        \"\"\"\n",
        "        variations = [word]\n",
        "\n",
        "        for prefix, alternates in self.prefix_variations.items():\n",
        "            if word.startswith(prefix):\n",
        "                for alt_prefix in alternates:\n",
        "                    if prefix != alt_prefix:\n",
        "                        variation = alt_prefix + word[len(prefix):]\n",
        "                        variations.append(variation)\n",
        "\n",
        "        return variations\n",
        "\n",
        "    def find_corrections(self, word: str, limit: int = 5, threshold: int = 70) -> List[Tuple[str, int]]:\n",
        "        \"\"\"\n",
        "        Enhanced correction finding with prefix variations and multiple similarity metrics\n",
        "        \"\"\"\n",
        "        # Check if word is already correct\n",
        "        if word in self.stopwords or word in self.dictionary:\n",
        "            return [(word, 100)]\n",
        "\n",
        "        # Generate prefix variations to check\n",
        "        word_variations = self._generate_prefix_variations(word)\n",
        "\n",
        "        # Comprehensive similarity calculation\n",
        "        candidates = []\n",
        "        for dict_word in self.dictionary:\n",
        "            for variation in word_variations:\n",
        "                # Stem both variation and dictionary word\n",
        "                stemmed_variation = self._advanced_stemmer(variation)\n",
        "                stemmed_dict_word = self._advanced_stemmer(dict_word)\n",
        "\n",
        "                # Multiple similarity metrics\n",
        "                phonetic_similarity = fuzz.ratio(\n",
        "                    self._phonetic_key(stemmed_variation),\n",
        "                    self._phonetic_key(stemmed_dict_word)\n",
        "                )\n",
        "\n",
        "                string_similarity = fuzz.ratio(stemmed_variation, stemmed_dict_word)\n",
        "                edit_similarity = fuzz.token_sort_ratio(stemmed_variation, stemmed_dict_word)\n",
        "\n",
        "                # Sequence matcher for more nuanced similarity\n",
        "                seq_matcher = difflib.SequenceMatcher(None, stemmed_variation, stemmed_dict_word)\n",
        "                sequence_similarity = seq_matcher.ratio() * 100\n",
        "\n",
        "                # Prefix similarity\n",
        "                prefix_similarity = fuzz.ratio(variation[:3], dict_word[:3]) * 0.5\n",
        "\n",
        "                # Combined weighted similarity\n",
        "                combined_score = (\n",
        "                    0.25 * phonetic_similarity +\n",
        "                    0.2 * string_similarity +\n",
        "                    0.15 * edit_similarity +\n",
        "                    0.25 * sequence_similarity +\n",
        "                    0.15 * prefix_similarity\n",
        "                )\n",
        "\n",
        "                candidates.append((dict_word, combined_score))\n",
        "\n",
        "        # Sort, filter, and limit candidates\n",
        "        candidates = sorted(candidates, key=lambda x: x[1], reverse=True)\n",
        "        unique_candidates = []\n",
        "        seen = set()\n",
        "        for candidate, score in candidates:\n",
        "            if candidate not in seen and score >= threshold:\n",
        "                unique_candidates.append((candidate, score))\n",
        "                seen.add(candidate)\n",
        "                if len(unique_candidates) == limit:\n",
        "                    break\n",
        "\n",
        "        return unique_candidates\n",
        "\n",
        "    def spell_check(self, text: str) -> Dict[str, List[Tuple[str, int]]]:\n",
        "        \"\"\"\n",
        "        Comprehensive spell checking\n",
        "        \"\"\"\n",
        "        words = re.findall(r'\\S+', text)\n",
        "\n",
        "        spelling_errors = {}\n",
        "        for word in words:\n",
        "            if word not in self.dictionary and word not in self.stopwords:\n",
        "                corrections = self.find_corrections(word)\n",
        "                if corrections:\n",
        "                    spelling_errors[word] = corrections\n",
        "\n",
        "        return spelling_errors\n",
        "\n",
        "    def auto_correct(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Automatically correct text using best suggestions\n",
        "        \"\"\"\n",
        "        errors = self.spell_check(text)\n",
        "        corrected_words = []\n",
        "\n",
        "        for word in text.split():\n",
        "            if word in errors:\n",
        "                corrected_words.append(errors[word][0][0])  # Take the first suggestion\n",
        "            else:\n",
        "                corrected_words.append(word)\n",
        "\n",
        "        return ' '.join(corrected_words)\n",
        "\n",
        "# Example Usage\n",
        "def main():\n",
        "    # Initialize spell checker with suffix list file path\n",
        "    spell_checker = SinhalaSpellChecker(\n",
        "        dictionary_path='/content/drive/MyDrive/UOJ/SEM 7/AI/Sinhala_Grammar_and_Spell_Checker/data-spell-checker.xlsx',\n",
        "        stopwords_path='/content/drive/MyDrive/UOJ/SEM 7/AI/Sinhala_Grammar_and_Spell_Checker/stop words.txt',\n",
        "        suffixes_path='/content/drive/MyDrive/UOJ/SEM 7/AI/Sinhala_Grammar_and_Spell_Checker/suffixes_list.txt'\n",
        "    )\n",
        "\n",
        "    test_texts = [\n",
        "        \"අභිචෝදකයා නඩුව සාක්ෂි ලබාදුන්නා. අංකනය කිරීම සඳහා කාර්යමණ්ඩලය වැඩි වේලාවක් ගත කළා. අංකාන්තරය සොයා ගැනීම සඳහා විශේෂ පරීක්ෂණයක කරනු ලැබුවා.\",\n",
        "        \"මම කඩේට ගියෙමු. අප සෙල්ලම් කළෝය.\",\n",
        "        \"උස ගසක් තිබෙයි. කුරුල්ලා ඒ මත කූඩුවක් තමයි. කුරුලු පැටව කෑම කයි\",\n",
        "        \"පාර්ලිමේන්තුවේ අද විශෂ සාකච්ඡාව පැවැත්විණි. පාර්ලිමේනතු මන්ත්‍රීවරයා ජාතික ප්‍රශ්න කතාබහ කළා.\",\n",
        "        \"මගේ ඉඩම විශාල ප්‍රමාණයකි. මගේ ගෘහ නිවසයේ වැඩක් ඉටු කළා.\"\n",
        "    ]\n",
        "\n",
        "    for text in test_texts:\n",
        "        print(\"\\n--- Spell Check for:\", text)\n",
        "\n",
        "        # Find spelling errors\n",
        "        errors = spell_checker.spell_check(text)\n",
        "        print(\"Spelling Errors:\", errors)\n",
        "\n",
        "        # Auto-correction\n",
        "        corrected_text = spell_checker.auto_correct(text)\n",
        "        print(\"Auto-corrected Text:\", corrected_text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "3dadc88f-3f68-4f92-9df6-8d2de6050975",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dadc88f-3f68-4f92-9df6-8d2de6050975",
        "outputId": "48fd84cd-2a27-42c5-9f19-6d93b3e50105"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Spell Check for: අපි බත් කනව\n",
            "Spelling Errors: {'කනව': [('කනවා', 82.62857142857143), ('කඩනවා', 78.05357142857143), ('කන', 74.0), ('කව', 74.0), ('නව', 74.0)]}\n",
            "Auto-corrected Text: අපි බත් කනවා\n"
          ]
        }
      ],
      "source": [
        "class SinhalaSpellChecker:\n",
        "    def __init__(self, dictionary_path: str, stopwords_path: str, suffixes_path: str, stem_dictionary_path: str):\n",
        "        # Load dictionary\n",
        "        self.data = pd.read_excel(dictionary_path)\n",
        "        self.dictionary = self._preprocess_dictionary()\n",
        "\n",
        "        # Load stopwords\n",
        "        with open(stopwords_path, 'r', encoding='utf-8') as file:\n",
        "            self.stopwords = set(file.read().splitlines())\n",
        "\n",
        "        # Load suffixes from suffixes_list.txt\n",
        "        with open(suffixes_path, 'r', encoding='utf-8') as file:\n",
        "            self.suffix_rules = file.read().splitlines()\n",
        "\n",
        "        # Load stem dictionary\n",
        "        self.stem_dictionary = self._load_stem_dictionary(stem_dictionary_path)\n",
        "\n",
        "        # Advanced phonetic mapping\n",
        "        self.phonetic_mapping = self._create_advanced_phonetic_mapping()\n",
        "\n",
        "        # Prefix and suffix variations\n",
        "        self.prefix_variations = {\n",
        "            'අ': ['ආ', 'අ'],\n",
        "            'අද': ['ආද', 'අද'],\n",
        "            'අන': ['ආන', 'අන']\n",
        "        }\n",
        "\n",
        "    def _load_stem_dictionary(self, stem_dictionary_path: str) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Load stem dictionary from a file, where each line contains a word variation and its stem.\n",
        "        \"\"\"\n",
        "        stem_dict = {}\n",
        "        with open(stem_dictionary_path, 'r', encoding='utf-8') as file:\n",
        "            for line in file:\n",
        "                word, stem = line.strip().split('\\t')\n",
        "                stem_dict[word] = stem\n",
        "        return stem_dict\n",
        "\n",
        "    def _preprocess_dictionary(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        Advanced dictionary preprocessing\n",
        "        \"\"\"\n",
        "        correct_words = self.data[self.data['label'] == 1]['word']\n",
        "\n",
        "        # Remove duplicates, convert to lowercase, remove special characters\n",
        "        processed_words = set(\n",
        "            re.sub(r'[^\\u0D80-\\u0DFF]', '', word.lower())\n",
        "            for word in correct_words\n",
        "        )\n",
        "\n",
        "        return list(processed_words)\n",
        "\n",
        "    def _create_advanced_phonetic_mapping(self) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Comprehensive phonetic mapping for Sinhala characters\n",
        "        \"\"\"\n",
        "        return {\n",
        "            # Consonant groups with similar sounds\n",
        "            'ක': 'k', 'ඛ': 'k', 'ගෑ': 'g', 'ඝ': 'g',\n",
        "            'ච': 'c', 'ජ': 'j', 'ඣ': 'j',\n",
        "            'ට': 't', 'ඩ': 'd', 'ඨ': 't', 'ඪ': 'd',\n",
        "            'ත': 't', 'ද': 'd', 'ධ': 'd',\n",
        "            'ප': 'p', 'බ': 'b', 'භ': 'b',\n",
        "            'ම': 'm', 'න': 'n', 'ණ': 'n',\n",
        "            'ල': 'l', 'ළ': 'l',\n",
        "            'ර': 'r', 'ඍ': 'r',\n",
        "            'ව': 'v', 'ශ': 's', 'ෂ': 's', 'ස': 's',\n",
        "            'හ': 'h'\n",
        "        }\n",
        "\n",
        "    def _advanced_stemmer(self, word: str) -> str:\n",
        "        \"\"\"\n",
        "        Advanced stemming with multiple suffix removal strategies and stem dictionary\n",
        "        \"\"\"\n",
        "        # First, check if the word exists in the stem dictionary\n",
        "        if word in self.stem_dictionary:\n",
        "            return self.stem_dictionary[word]\n",
        "\n",
        "        # If not, apply suffix removal rules\n",
        "        original_word = word\n",
        "        for suffix in self.suffix_rules:\n",
        "            if word.endswith(suffix):\n",
        "                word = word[:-len(suffix)]\n",
        "                break\n",
        "\n",
        "        # If no suffix removed and word is too short, return original\n",
        "        return word if len(word) > 2 else original_word\n",
        "\n",
        "    def _phonetic_key(self, word: str) -> str:\n",
        "        \"\"\"\n",
        "        Generate advanced phonetic key\n",
        "        \"\"\"\n",
        "        phonetic_key = ''\n",
        "        for char in word:\n",
        "            phonetic_key += self.phonetic_mapping.get(char, char)\n",
        "        return phonetic_key\n",
        "\n",
        "    def _generate_prefix_variations(self, word: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Generate potential prefix variations of a word\n",
        "        \"\"\"\n",
        "        variations = [word]\n",
        "\n",
        "        for prefix, alternates in self.prefix_variations.items():\n",
        "            if word.startswith(prefix):\n",
        "                for alt_prefix in alternates:\n",
        "                    if prefix != alt_prefix:\n",
        "                        variation = alt_prefix + word[len(prefix):]\n",
        "                        variations.append(variation)\n",
        "\n",
        "        return variations\n",
        "\n",
        "    def find_corrections(self, word: str, limit: int = 5, threshold: int = 70) -> List[Tuple[str, int]]:\n",
        "        \"\"\"\n",
        "        Enhanced correction finding with prefix variations and multiple similarity metrics\n",
        "        \"\"\"\n",
        "        # Check if word is already correct\n",
        "        if word in self.stopwords or word in self.dictionary:\n",
        "            return [(word, 100)]\n",
        "\n",
        "        # Generate prefix variations to check\n",
        "        word_variations = self._generate_prefix_variations(word)\n",
        "\n",
        "        # Comprehensive similarity calculation\n",
        "        candidates = []\n",
        "        for dict_word in self.dictionary:\n",
        "            for variation in word_variations:\n",
        "                # Stem both variation and dictionary word\n",
        "                stemmed_variation = self._advanced_stemmer(variation)\n",
        "                stemmed_dict_word = self._advanced_stemmer(dict_word)\n",
        "\n",
        "                # Multiple similarity metrics\n",
        "                phonetic_similarity = fuzz.ratio(\n",
        "                    self._phonetic_key(stemmed_variation),\n",
        "                    self._phonetic_key(stemmed_dict_word)\n",
        "                )\n",
        "\n",
        "                string_similarity = fuzz.ratio(stemmed_variation, stemmed_dict_word)\n",
        "                edit_similarity = fuzz.token_sort_ratio(stemmed_variation, stemmed_dict_word)\n",
        "\n",
        "                # Sequence matcher for more nuanced similarity\n",
        "                seq_matcher = difflib.SequenceMatcher(None, stemmed_variation, stemmed_dict_word)\n",
        "                sequence_similarity = seq_matcher.ratio() * 100\n",
        "\n",
        "                # Prefix similarity\n",
        "                prefix_similarity = fuzz.ratio(variation[:3], dict_word[:3]) * 0.5\n",
        "\n",
        "                # Combined weighted similarity\n",
        "                combined_score = (\n",
        "                    0.25 * phonetic_similarity +\n",
        "                    0.2 * string_similarity +\n",
        "                    0.15 * edit_similarity +\n",
        "                    0.25 * sequence_similarity +\n",
        "                    0.15 * prefix_similarity\n",
        "                )\n",
        "\n",
        "                candidates.append((dict_word, combined_score))\n",
        "\n",
        "        # Sort, filter, and limit candidates\n",
        "        candidates = sorted(candidates, key=lambda x: x[1], reverse=True)\n",
        "        unique_candidates = []\n",
        "        seen = set()\n",
        "        for candidate, score in candidates:\n",
        "            if candidate not in seen and score >= threshold:\n",
        "                unique_candidates.append((candidate, score))\n",
        "                seen.add(candidate)\n",
        "                if len(unique_candidates) == limit:\n",
        "                    break\n",
        "\n",
        "        return unique_candidates\n",
        "\n",
        "    def spell_check(self, text: str) -> Dict[str, List[Tuple[str, int]]]:\n",
        "        \"\"\"\n",
        "        Comprehensive spell checking\n",
        "        \"\"\"\n",
        "        words = re.findall(r'\\S+', text)\n",
        "\n",
        "        spelling_errors = {}\n",
        "        for word in words:\n",
        "            if word not in self.dictionary and word not in self.stopwords:\n",
        "                corrections = self.find_corrections(word)\n",
        "                if corrections:\n",
        "                    spelling_errors[word] = corrections\n",
        "\n",
        "        return spelling_errors\n",
        "\n",
        "    def auto_correct(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Automatically correct text using best suggestions\n",
        "        \"\"\"\n",
        "        errors = self.spell_check(text)\n",
        "        corrected_words = []\n",
        "\n",
        "        for word in text.split():\n",
        "            if word in errors:\n",
        "                corrected_words.append(errors[word][0][0])  # Take the first suggestion\n",
        "            else:\n",
        "                corrected_words.append(word)\n",
        "\n",
        "        return ' '.join(corrected_words)\n",
        "\n",
        "# Example Usage\n",
        "def main():\n",
        "    # Initialize spell checker with suffix list and stem dictionary file paths\n",
        "    spell_checker = SinhalaSpellChecker(\n",
        "        dictionary_path='/content/drive/MyDrive/UOJ/SEM 7/AI/Sinhala_Grammar_and_Spell_Checker/data-spell-checker.xlsx',\n",
        "        stopwords_path='/content/drive/MyDrive/UOJ/SEM 7/AI/Sinhala_Grammar_and_Spell_Checker/stop words.txt',\n",
        "        suffixes_path='/content/drive/MyDrive/UOJ/SEM 7/AI/Sinhala_Grammar_and_Spell_Checker/suffixes_list.txt',\n",
        "        stem_dictionary_path='/content/drive/MyDrive/UOJ/SEM 7/AI/Sinhala_Grammar_and_Spell_Checker/stem_dictionary.txt'\n",
        "    )\n",
        "\n",
        "    test_texts = [\n",
        "        \"අපි බත් කනව\"\n",
        "    ]\n",
        "\n",
        "    for text in test_texts:\n",
        "        print(\"\\n--- Spell Check for:\", text)\n",
        "\n",
        "        # Find spelling errors\n",
        "        errors = spell_checker.spell_check(text)\n",
        "        print(\"Spelling Errors:\", errors)\n",
        "\n",
        "        # Auto-correction\n",
        "        corrected_text = spell_checker.auto_correct(text)\n",
        "        print(\"Auto-corrected Text:\", corrected_text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "d52a1d32-4171-4b96-8c6c-a724eff5d35b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d52a1d32-4171-4b96-8c6c-a724eff5d35b",
        "outputId": "98f92fa3-960d-42e2-8239-1b33abddd290"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grammar error: Verb '.' should end with 'මු' when the subject is 'අපි'.\n"
          ]
        }
      ],
      "source": [
        "from sinling import SinhalaTokenizer, POSTagger\n",
        "\n",
        "class SinhalaGrammarChecker:\n",
        "    def __init__(self):\n",
        "        self.tokenizer = SinhalaTokenizer()\n",
        "        self.tagger = POSTagger()\n",
        "\n",
        "    def is_sov_order(self, pos_tags):\n",
        "        \"\"\"\n",
        "        Check if the sentence follows SOV order based on POS tags.\n",
        "        \"\"\"\n",
        "        if len(pos_tags) < 3:\n",
        "            return False  # Sentence too short to be SOV\n",
        "\n",
        "        subject_tag, object_tag, verb_tag = pos_tags[0][1], pos_tags[1][1], pos_tags[2][1]\n",
        "\n",
        "        # SOV structure: S -> PRP, O -> NNC, V -> V* (verbs starting with 'V')\n",
        "        return subject_tag == 'PRP' and object_tag == 'NNC' and verb_tag.startswith('V')\n",
        "\n",
        "    def check_grammar(self, sentence):\n",
        "        \"\"\"\n",
        "        Check grammar rules for a given sentence.\n",
        "        \"\"\"\n",
        "        tokenized_sentences = [self.tokenizer.tokenize(f'{ss}.') for ss in self.tokenizer.split_sentences(sentence)]\n",
        "        pos_tags = self.tagger.predict(tokenized_sentences)\n",
        "\n",
        "        if not pos_tags or not pos_tags[0]:\n",
        "            return \"Unable to analyze the sentence.\"\n",
        "\n",
        "        tokens = tokenized_sentences[0]\n",
        "        tags = pos_tags[0]\n",
        "\n",
        "        # Ensure the sentence follows SOV structure\n",
        "        if not self.is_sov_order(tags):\n",
        "            return \"Sentence does not follow SOV order.\"\n",
        "\n",
        "        # Extract Subject, Verb, and Object\n",
        "        subject = tokens[0]\n",
        "        verb = tokens[-1]\n",
        "\n",
        "        # Rule 1: If S = \"මම\", V should end with \"මි\"\n",
        "        if subject == \"මම\" and not verb.endswith(\"මි\"):\n",
        "            return f\"Grammar error: Verb '{verb}' should end with 'මි' when the subject is 'මම'.\"\n",
        "\n",
        "        # Rule 2: If S = \"අපි\", V should end with \"මු\"\n",
        "        if subject == \"අපි\" and not verb.endswith(\"මු\"):\n",
        "            return f\"Grammar error: Verb '{verb}' should end with 'මු' when the subject is 'අපි'.\"\n",
        "\n",
        "        return \"The sentence is grammatically correct.\"\n",
        "\n",
        "# Example Usage\n",
        "def main():\n",
        "    grammar_checker = SinhalaGrammarChecker()\n",
        "\n",
        "    sentence = \"අපි බත් කනවා\"\n",
        "    result = grammar_checker.check_grammar(sentence)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "b610e661-8cce-479b-a931-fdeafa7dbe3b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b610e661-8cce-479b-a931-fdeafa7dbe3b",
        "outputId": "7a9a92f5-16f5-4794-9588-7ff910aedeb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analyzing: අපි පොත කියවමු\n",
            "✓ Grammatically correct\n",
            "\n",
            "Analyzing: මම පාඩම් කරයි. මම කඩේට යමි.\n",
            "✗ Grammar errors found:\n",
            "  - Sentence does not follow Subject-Object-Verb order\n",
            "  - Verb ending doesn't agree with subject 'මම'\n",
            "\n",
            "Suggestions:\n",
            "  - Consider: මම පාඩම් කරමි\n",
            "\n",
            "Analyzing: ඔවුන් අඹ කයි. අයියා පාසල් යයි.\n",
            "✗ Grammar errors found:\n",
            "  - Sentence does not follow Subject-Object-Verb order\n",
            "  - Verb ending doesn't agree with subject 'ඔවුන්'\n",
            "\n",
            "Suggestions:\n",
            "  - Consider: ඔවුන් අඹ කති\n",
            "\n",
            "Analyzing: අපි පාසලට ගියෙම. ගුරුවරුන් අපට පාඩම් උගන්වති. දරුවෝ අකුරු උගනි.\n",
            "✗ Grammar errors found:\n",
            "  - Sentence does not follow Subject-Object-Verb order\n",
            "  - Verb ending doesn't agree with subject 'අපි'\n",
            "\n",
            "Suggestions:\n",
            "  - Consider: අපි පාසලට ගියමු\n",
            "\n",
            "Analyzing: ඇය සතුටින් ඉඳිමු. මම කඩේට ගියෙමි. අපි පොත කියවමු.\n",
            "✗ Grammar errors found:\n",
            "  - Verb ending doesn't agree with subject 'ඇය'\n",
            "\n",
            "Suggestions:\n",
            "  - Consider: ඇය සතුටින් ඉඳියි\n",
            "\n",
            "Analyzing: ඔහු සිංහල ඉගෙනගනී. අම්මා උදෑසනම රැකියාවට ගියාය.\n",
            "✗ Grammar errors found:\n",
            "  - Sentence does not follow Subject-Object-Verb order\n",
            "  - Verb ending doesn't agree with subject 'ඔහු'\n",
            "\n",
            "Suggestions:\n",
            "  - Consider: ඔහු සිංහල ඉගෙනගයි\n"
          ]
        }
      ],
      "source": [
        "from sinling import SinhalaTokenizer, POSTagger\n",
        "from typing import List, Tuple, Optional\n",
        "\n",
        "class SinhalaGrammarChecker:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the grammar checker with required tools.\"\"\"\n",
        "        self.tokenizer = SinhalaTokenizer()\n",
        "        self.tagger = POSTagger()\n",
        "\n",
        "        # Define verb endings for different subjects\n",
        "        self.subject_verb_endings = {\n",
        "            \"මම\": \"මි\",    # I\n",
        "            \"අපි\": \"මු\",    # We\n",
        "            \"ඔහු\": \"යි\",    # He\n",
        "            \"ඇය\": \"යි\",    # She\n",
        "            \"ඔවුන්\": \"ති\",  # They\n",
        "            \"ඔබ\": \"හි\",     # You (singular)\n",
        "            \"ඔබලා\": \"හු\",   # You (plural)\n",
        "        }\n",
        "\n",
        "    def tokenize_and_tag(self, sentence: str) -> Tuple[List[str], List[Tuple[str, str]]]:\n",
        "        \"\"\"\n",
        "        Tokenize and POS tag the input sentence.\n",
        "\n",
        "        Args:\n",
        "            sentence: Input Sinhala sentence\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (tokens, pos_tags)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Add period if sentence doesn't end with punctuation\n",
        "            if not sentence[-1] in [\".\", \"!\", \"?\"]:\n",
        "                sentence += \".\"\n",
        "\n",
        "            tokenized_sentences = [self.tokenizer.tokenize(ss)\n",
        "            for ss in self.tokenizer.split_sentences(sentence)]\n",
        "            if not tokenized_sentences:\n",
        "                raise ValueError(\"Empty sentence after tokenization\")\n",
        "\n",
        "            pos_tags = self.tagger.predict(tokenized_sentences)\n",
        "            if not pos_tags or not pos_tags[0]:\n",
        "                raise ValueError(\"Failed to generate POS tags\")\n",
        "\n",
        "            return tokenized_sentences[0], pos_tags[0]\n",
        "\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Error in tokenization/tagging: {str(e)}\")\n",
        "\n",
        "    def is_sov_order(self, tokens: List[str], pos_tags: List[Tuple[str, str]]) -> bool:\n",
        "        \"\"\"\n",
        "        Check if the sentence follows Subject-Object-Verb order.\n",
        "\n",
        "        Args:\n",
        "            tokens: List of tokenized words\n",
        "            pos_tags: List of POS tags for each token\n",
        "\n",
        "        Returns:\n",
        "            Boolean indicating if sentence follows SOV order\n",
        "        \"\"\"\n",
        "        if len(pos_tags) < 3:\n",
        "            return False\n",
        "\n",
        "        # Get basic components\n",
        "        subject_pos = pos_tags[0][1]\n",
        "        verb_pos = pos_tags[-1][1]\n",
        "\n",
        "        # Check if there's an object between subject and verb\n",
        "        has_object = False\n",
        "        for tag in pos_tags[1:-1]:\n",
        "            if tag[1] in ['NNC', 'NNP', 'PRP']:  # Common noun, proper noun, or pronoun\n",
        "                has_object = True\n",
        "                break\n",
        "\n",
        "        return (subject_pos in ['PRP', 'NNP', 'NNC'] and  # Subject is pronoun or noun\n",
        "                verb_pos.startswith('V') and  # Last word is verb\n",
        "                has_object)  # Has object between S and V\n",
        "\n",
        "    def check_subject_verb_agreement(self, subject: str, verb: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Check if the verb ending agrees with the subject.\n",
        "\n",
        "        Args:\n",
        "            subject: Subject word\n",
        "            verb: Verb word\n",
        "\n",
        "        Returns:\n",
        "            Correction suggestion if there's an error, None if correct\n",
        "        \"\"\"\n",
        "        if subject in self.subject_verb_endings:\n",
        "            expected_ending = self.subject_verb_endings[subject]\n",
        "            if not verb.endswith(expected_ending):\n",
        "                # Generate correct verb form\n",
        "                verb_root = verb[:-2] if len(verb) > 2 else verb\n",
        "                correct_verb = verb_root + expected_ending\n",
        "                return correct_verb\n",
        "        return None\n",
        "\n",
        "    def check_grammar(self, sentence: str) -> dict:\n",
        "        \"\"\"\n",
        "        Check grammar rules for a given sentence and return detailed analysis.\n",
        "\n",
        "        Args:\n",
        "            sentence: Input Sinhala sentence\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing analysis results and suggestions\n",
        "        \"\"\"\n",
        "        try:\n",
        "            tokens, pos_tags = self.tokenize_and_tag(sentence)\n",
        "\n",
        "            result = {\n",
        "                \"original\": sentence,\n",
        "                \"is_grammatical\": True,\n",
        "                \"errors\": [],\n",
        "                \"suggestions\": [],\n",
        "                \"analysis\": {\n",
        "                    \"tokens\": tokens,\n",
        "                    \"pos_tags\": pos_tags\n",
        "                }\n",
        "            }\n",
        "\n",
        "            # Check word order\n",
        "            if not self.is_sov_order(tokens, pos_tags):\n",
        "                result[\"is_grammatical\"] = False\n",
        "                result[\"errors\"].append(\"Sentence does not follow Subject-Object-Verb order\")\n",
        "                # Suggest correction by reordering\n",
        "                suggested_order = self._reorder_to_sov(tokens, pos_tags)\n",
        "                if suggested_order:\n",
        "                    result[\"suggestions\"].append(f\"Consider: {' '.join(suggested_order)}\")\n",
        "\n",
        "            # Check subject-verb agreement\n",
        "            subject = tokens[0]\n",
        "            verb = tokens[-1]\n",
        "            corrected_verb = self.check_subject_verb_agreement(subject, verb)\n",
        "\n",
        "            if corrected_verb:\n",
        "                result[\"is_grammatical\"] = False\n",
        "                result[\"errors\"].append(f\"Verb ending doesn't agree with subject '{subject}'\")\n",
        "                # Create corrected sentence\n",
        "                corrected_tokens = tokens[:-1] + [corrected_verb]\n",
        "                result[\"suggestions\"].append(f\"Consider: {' '.join(corrected_tokens)}\")\n",
        "\n",
        "            return result\n",
        "\n",
        "        except ValueError as e:\n",
        "            return {\n",
        "                \"original\": sentence,\n",
        "                \"is_grammatical\": False,\n",
        "                \"errors\": [str(e)],\n",
        "                \"suggestions\": [],\n",
        "                \"analysis\": None\n",
        "            }\n",
        "\n",
        "    def _reorder_to_sov(self, tokens: List[str], pos_tags: List[Tuple[str, str]]) -> Optional[List[str]]:\n",
        "        \"\"\"\n",
        "        Attempt to reorder tokens to follow SOV order.\n",
        "\n",
        "        Args:\n",
        "            tokens: List of tokens\n",
        "            pos_tags: List of POS tags\n",
        "\n",
        "        Returns:\n",
        "            Reordered list of tokens if possible, None if not\n",
        "        \"\"\"\n",
        "        # Find subject, object, and verb candidates\n",
        "        subject_idx = None\n",
        "        object_idx = None\n",
        "        verb_idx = None\n",
        "\n",
        "        for i, (token, tag) in enumerate(zip(tokens, pos_tags)):\n",
        "            if tag[1] in ['PRP', 'NNP', 'NNC'] and subject_idx is None:\n",
        "                subject_idx = i\n",
        "            elif tag[1] in ['NNC', 'NNP', 'PRP'] and subject_idx is not None and object_idx is None:\n",
        "                object_idx = i\n",
        "            elif tag[1].startswith('V'):\n",
        "                verb_idx = i\n",
        "\n",
        "        if all(x is not None for x in [subject_idx, object_idx, verb_idx]):\n",
        "            # Reorder maintaining other words' relative positions\n",
        "            reordered = []\n",
        "            # Add subject\n",
        "            reordered.append(tokens[subject_idx])\n",
        "            # Add object\n",
        "            reordered.append(tokens[object_idx])\n",
        "            # Add any intervening words\n",
        "            for i, token in enumerate(tokens):\n",
        "                if i not in [subject_idx, object_idx, verb_idx]:\n",
        "                    reordered.append(token)\n",
        "            # Add verb at end\n",
        "            reordered.append(tokens[verb_idx])\n",
        "            return reordered\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    \"\"\"Example usage of the grammar checker.\"\"\"\n",
        "    checker = SinhalaGrammarChecker()\n",
        "\n",
        "    # Test sentences\n",
        "    test_sentences = [\n",
        "        \"අපි පොත කියවමු\",\n",
        "        \"මම පාඩම් කරයි. මම කඩේට යමි.\",\n",
        "        \"ඔවුන් අඹ කයි. අයියා පාසල් යයි.\",\n",
        "        \"අපි පාසලට ගියෙම. ගුරුවරුන් අපට පාඩම් උගන්වති. දරුවෝ අකුරු උගනි.\",\n",
        "        \"ඇය සතුටින් ඉඳිමු. මම කඩේට ගියෙමි. අපි පොත කියවමු.\",\n",
        "        \"ඔහු සිංහල ඉගෙනගනී. අම්මා උදෑසනම රැකියාවට ගියාය.\"\n",
        "\n",
        "    ]\n",
        "\n",
        "    for sentence in test_sentences:\n",
        "        print(\"\\nAnalyzing:\", sentence)\n",
        "        result = checker.check_grammar(sentence)\n",
        "\n",
        "        if result[\"is_grammatical\"]:\n",
        "            print(\"✓ Grammatically correct\")\n",
        "        else:\n",
        "            print(\"✗ Grammar errors found:\")\n",
        "            for error in result[\"errors\"]:\n",
        "                print(f\"  - {error}\")\n",
        "            print(\"\\nSuggestions:\")\n",
        "            for suggestion in result[\"suggestions\"]:\n",
        "                print(f\"  - {suggestion}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}